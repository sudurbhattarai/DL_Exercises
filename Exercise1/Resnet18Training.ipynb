{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ee9fb9b-4627-4b40-ae2d-5deacfe39591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import pathlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fff42cf5-e179-48ef-ac83-64bdbde978b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_dir, labels, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.file_paths = []\n",
    "        self.label_map = {'not_car': 0, 'car': 1}\n",
    "        self.classes = labels\n",
    "        \n",
    "        for lab in self.labels:\n",
    "            label_folder = os.path.join(str(img_dir), lab)\n",
    "            file_list = [os.path.join(label_folder, file.name) for file in os.scandir(label_folder) if file.is_file()]\n",
    "            self.file_paths.extend([(file, lab) for file in file_list])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.file_paths[idx]\n",
    "        image = read_image(img_path, mode=ImageReadMode.RGB).float() / 255.0 # Scale to [0, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label_id = self.label_map[label]\n",
    "        return image, label_id\n",
    "        \n",
    "# Example usage\n",
    "train_dir = 'D:/MSRSGI/Summer_Semester24/DL/Exercises/Exercise1/WorkingData/data/train'\n",
    "test_dir = 'D:/MSRSGI/Summer_Semester24/DL/Exercises/Exercise1/WorkingData/data/test'\n",
    "labels = ['not_car', 'car']  # Ensure this matches your directory structure\n",
    "\n",
    "# Define the transforms\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224))\n",
    "    # transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(train_dir, labels, transform = data_transforms) \n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True) #takes an image not a tensor \n",
    "\n",
    "test_dataset = CustomDataset(test_dir, labels, transform = data_transforms) \n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=True) #takes an image not a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a96ee7f-3dd9-48b3-8c36-ae70f33546ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4200, 1346)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91c88355-9cd0-4596-9c53-2d821e5e31ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.4886, 0.4886, 0.4760]), tensor([0.1482, 0.1395, 0.1342]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_mean_std(loader):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    total_image_count = 0\n",
    "    for images, _ in loader:\n",
    "        image_count_in_a_batch = images.size(0)\n",
    "        images = images.view(image_count_in_a_batch, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        total_image_count += image_count_in_a_batch\n",
    "        \n",
    "    mean /= total_image_count\n",
    "    std /= total_image_count\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "# data_transforms = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "batch_size = 32\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "mean_train, std_train = get_mean_std(train_dataloader)\n",
    "mean_train, std_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7678fd3d-eee1-4deb-8507-4a1c85a7cf46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.4670, 0.4365, 0.4319]), tensor([0.1659, 0.1498, 0.1380]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "mean_test, std_test = get_mean_std(test_dataloader)\n",
    "mean_test, std_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11575c5a-f009-4b39-ab72-33812e33a7f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define data transformations for data augmentation and normalization\n",
    "train_transforms_norm = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    # transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4886, 0.4886, 0.4760], [0.1482, 0.1395, 0.1342]) #taken these values from: mean_train, std_train = get_mean_std(train_dataloader)\n",
    "])\n",
    "\n",
    "train_dataset_norm = CustomDataset(train_dir, labels, transform = train_transforms_norm)\n",
    "len(train_dataset_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14c4258c-69e9-4b8c-a20d-d3d0b3b49053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1346"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_transforms_norm = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    # transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4670, 0.4365, 0.4319], [0.1659, 0.1498, 0.1380])\n",
    "])\n",
    "\n",
    "# dataset = datasets.ImageFolder(data_dir, transform=data_transforms)\n",
    "\n",
    "test_dataset_norm = CustomDataset(test_dir, labels, transform = test_transforms_norm) \n",
    "len(test_dataset_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b905160c-9663-4cea-bdc9-566fc6c8f529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader_norm = torch.utils.data.DataLoader(dataset=train_dataset_norm, batch_size=32, shuffle=True) #takes an image not a tensor \n",
    "len(train_dataloader_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3b89a86-de08-4559-ab03-7862a717588e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader_norm = torch.utils.data.DataLoader(dataset=test_dataset_norm, batch_size=32, shuffle=True) #takes an image not a tensor \n",
    "len(test_dataloader_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bddd83e-4693-4297-a623-ac753573a7dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not_car', 'car']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = train_dataset.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dc2ffd8-b97c-4ce5-9dc8-5eb38143b812",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_norm = CustomDataset(train_dir, labels, transform = train_transforms_norm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1b33c11-4c23-4623-9b8b-32e7777ecc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sudur\\miniconda3\\envs\\DeepLearning\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sudur\\miniconda3\\envs\\DeepLearning\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet18\n",
    "# Load the pre-trained ResNet18 model\n",
    "model = resnet18(pretrained=True)\n",
    "num_classes = len(train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfdccfcb-9bcd-42e1-b853-c65cc988ab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers except the final classification layer\n",
    "for name, param in model.named_parameters():\n",
    "    if \"fc\" in name:  # Unfreeze the final classification layer\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "931d9fba-a585-4762-a8af-85944c779b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= 0.0001)  # Use all parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba3fd09c-e028-47f1-b5c2-0721c8c2b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a606e8e-4696-4971-a067-a389a198a88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 7.7711  Accuracy: 0.0169\n",
      "Minibatch Loss: 5.9939  Accuracy: 0.0991\n",
      "Loss: 5.1470  Accuracy: 0.1598\n",
      "Epoch 2/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 1.6311  Accuracy: 0.4738\n",
      "Minibatch Loss: 1.4304  Accuracy: 0.5088\n",
      "Loss: 1.3379  Accuracy: 0.5348\n",
      "Epoch 3/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.9385  Accuracy: 0.6288\n",
      "Minibatch Loss: 0.9018  Accuracy: 0.6312\n",
      "Loss: 0.8637  Accuracy: 0.6448\n",
      "Epoch 4/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.7415  Accuracy: 0.6875\n",
      "Minibatch Loss: 0.6996  Accuracy: 0.6987\n",
      "Loss: 0.6854  Accuracy: 0.7076\n",
      "Epoch 5/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.5743  Accuracy: 0.7688\n",
      "Minibatch Loss: 0.5584  Accuracy: 0.7594\n",
      "Loss: 0.5573  Accuracy: 0.7595\n",
      "Epoch 6/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.4782  Accuracy: 0.7987\n",
      "Minibatch Loss: 0.4656  Accuracy: 0.8031\n",
      "Loss: 0.4632  Accuracy: 0.8019\n",
      "Epoch 7/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.4317  Accuracy: 0.8219\n",
      "Minibatch Loss: 0.4143  Accuracy: 0.8284\n",
      "Loss: 0.4036  Accuracy: 0.8343\n",
      "Epoch 8/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.3501  Accuracy: 0.8525\n",
      "Minibatch Loss: 0.3738  Accuracy: 0.8438\n",
      "Loss: 0.3636  Accuracy: 0.8474\n",
      "Epoch 9/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.3214  Accuracy: 0.8731\n",
      "Minibatch Loss: 0.3235  Accuracy: 0.8747\n",
      "Loss: 0.3249  Accuracy: 0.8707\n",
      "Epoch 10/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.3058  Accuracy: 0.8750\n",
      "Minibatch Loss: 0.2814  Accuracy: 0.8888\n",
      "Loss: 0.2854  Accuracy: 0.8843\n",
      "Epoch 11/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.2655  Accuracy: 0.8919\n",
      "Minibatch Loss: 0.2689  Accuracy: 0.8931\n",
      "Loss: 0.2652  Accuracy: 0.8940\n",
      "Epoch 12/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.2451  Accuracy: 0.9069\n",
      "Minibatch Loss: 0.2468  Accuracy: 0.9044\n",
      "Loss: 0.2428  Accuracy: 0.9062\n",
      "Epoch 13/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.2261  Accuracy: 0.9163\n",
      "Minibatch Loss: 0.2299  Accuracy: 0.9131\n",
      "Loss: 0.2318  Accuracy: 0.9105\n",
      "Epoch 14/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.2133  Accuracy: 0.9175\n",
      "Minibatch Loss: 0.2134  Accuracy: 0.9194\n",
      "Loss: 0.2091  Accuracy: 0.9210\n",
      "Epoch 15/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.1989  Accuracy: 0.9225\n",
      "Minibatch Loss: 0.1942  Accuracy: 0.9297\n",
      "Loss: 0.1956  Accuracy: 0.9290\n",
      "Epoch 16/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.1972  Accuracy: 0.9231\n",
      "Minibatch Loss: 0.1913  Accuracy: 0.9259\n",
      "Loss: 0.1887  Accuracy: 0.9276\n",
      "Epoch 17/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.1779  Accuracy: 0.9344\n",
      "Minibatch Loss: 0.1808  Accuracy: 0.9325\n",
      "Loss: 0.1827  Accuracy: 0.9314\n",
      "Epoch 18/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.1749  Accuracy: 0.9375\n",
      "Minibatch Loss: 0.1792  Accuracy: 0.9316\n",
      "Loss: 0.1707  Accuracy: 0.9364\n",
      "Epoch 19/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.1657  Accuracy: 0.9337\n",
      "Minibatch Loss: 0.1665  Accuracy: 0.9337\n",
      "Loss: 0.1652  Accuracy: 0.9371\n",
      "Epoch 20/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.1618  Accuracy: 0.9413\n",
      "Minibatch Loss: 0.1644  Accuracy: 0.9391\n",
      "Loss: 0.1599  Accuracy: 0.9402\n",
      "Epoch 21/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.1604  Accuracy: 0.9444\n",
      "Minibatch Loss: 0.1584  Accuracy: 0.9463\n",
      "Loss: 0.1576  Accuracy: 0.9467\n",
      "Epoch 22/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.1532  Accuracy: 0.9413\n",
      "Minibatch Loss: 0.1499  Accuracy: 0.9428\n",
      "Loss: 0.1470  Accuracy: 0.9457\n",
      "Epoch 23/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.1471  Accuracy: 0.9394\n",
      "Minibatch Loss: 0.1419  Accuracy: 0.9472\n",
      "Loss: 0.1411  Accuracy: 0.9479\n",
      "Epoch 24/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.1413  Accuracy: 0.9513\n",
      "Minibatch Loss: 0.1440  Accuracy: 0.9487\n",
      "Loss: 0.1442  Accuracy: 0.9471\n",
      "Epoch 25/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.1361  Accuracy: 0.9487\n",
      "Minibatch Loss: 0.1332  Accuracy: 0.9525\n",
      "Loss: 0.1328  Accuracy: 0.9519\n",
      "Epoch 26/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.1357  Accuracy: 0.9506\n",
      "Minibatch Loss: 0.1305  Accuracy: 0.9534\n",
      "Loss: 0.1274  Accuracy: 0.9533\n",
      "Epoch 27/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.1203  Accuracy: 0.9537\n",
      "Minibatch Loss: 0.1344  Accuracy: 0.9516\n",
      "Loss: 0.1342  Accuracy: 0.9519\n",
      "Epoch 28/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.1259  Accuracy: 0.9537\n",
      "Minibatch Loss: 0.1185  Accuracy: 0.9594\n",
      "Loss: 0.1226  Accuracy: 0.9574\n",
      "Epoch 29/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.1295  Accuracy: 0.9569\n",
      "Minibatch Loss: 0.1218  Accuracy: 0.9581\n",
      "Loss: 0.1200  Accuracy: 0.9581\n",
      "Epoch 30/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.1202  Accuracy: 0.9581\n",
      "Minibatch Loss: 0.1145  Accuracy: 0.9603\n",
      "Loss: 0.1144  Accuracy: 0.9590\n",
      "Epoch 31/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.1178  Accuracy: 0.9619\n",
      "Minibatch Loss: 0.1158  Accuracy: 0.9641\n",
      "Loss: 0.1140  Accuracy: 0.9638\n",
      "Epoch 32/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.1050  Accuracy: 0.9637\n",
      "Minibatch Loss: 0.1091  Accuracy: 0.9603\n",
      "Loss: 0.1097  Accuracy: 0.9612\n",
      "Epoch 33/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.1173  Accuracy: 0.9600\n",
      "Minibatch Loss: 0.1114  Accuracy: 0.9606\n",
      "Loss: 0.1113  Accuracy: 0.9607\n",
      "Epoch 34/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.1046  Accuracy: 0.9669\n",
      "Minibatch Loss: 0.1028  Accuracy: 0.9653\n",
      "Loss: 0.0995  Accuracy: 0.9664\n",
      "Epoch 35/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.0979  Accuracy: 0.9644\n",
      "Minibatch Loss: 0.1037  Accuracy: 0.9631\n",
      "Loss: 0.1037  Accuracy: 0.9633\n",
      "Epoch 36/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.1038  Accuracy: 0.9656\n",
      "Minibatch Loss: 0.1032  Accuracy: 0.9650\n",
      "Loss: 0.0984  Accuracy: 0.9669\n",
      "Epoch 37/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.0901  Accuracy: 0.9725\n",
      "Minibatch Loss: 0.0898  Accuracy: 0.9722\n",
      "Loss: 0.0943  Accuracy: 0.9700\n",
      "Epoch 38/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.1071  Accuracy: 0.9594\n",
      "Minibatch Loss: 0.1017  Accuracy: 0.9622\n",
      "Loss: 0.1018  Accuracy: 0.9633\n",
      "Epoch 39/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.1038  Accuracy: 0.9613\n",
      "Minibatch Loss: 0.1041  Accuracy: 0.9631\n",
      "Loss: 0.0978  Accuracy: 0.9650\n",
      "Epoch 40/40\n",
      "--------------------------------------------------\n",
      "Minibatch Loss: 0.0852  Accuracy: 0.9738\n",
      "Minibatch Loss: 0.0892  Accuracy: 0.9722\n",
      "Loss: 0.0969  Accuracy: 0.9688\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 40\n",
    "batch_size = 32\n",
    "\n",
    "best_accuracy=0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    #Evaluation and training on training dataset\n",
    "    model.train()\n",
    "    train_accuracy=0.0\n",
    "    train_loss=0.0\n",
    "    total_samples = 0.0\n",
    "    \n",
    "    for i, (images,labels) in enumerate(train_dataloader_norm):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "                            \n",
    "        optimizer.zero_grad()\n",
    "                            \n",
    "        outputs=model(images)\n",
    "        loss=criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss+= loss.cpu().data*images.size(0)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        \n",
    "        train_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        # Print mini-batch statistics\n",
    "        if (i + 1) % 50 == 0:\n",
    "            mini_batch_loss = train_loss / total_samples\n",
    "            mini_batch_accuracy = train_accuracy / total_samples\n",
    "            print(f\"Minibatch Loss: {mini_batch_loss:.4f}  Accuracy: {mini_batch_accuracy:.4f}\")\n",
    "    \n",
    "    # Calculate average loss and accuracy for the epoch\n",
    "    epoch_loss = train_loss / len(train_dataloader_norm.dataset)\n",
    "    epoch_accuracy = train_accuracy / len(train_dataloader_norm.dataset)\n",
    "    \n",
    "    # Evaluation on testing dataset\n",
    "    model.eval()\n",
    "    test_accuracy=0.0\n",
    "    for i, (images,labels) in enumerate(test_dataloader_norm):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "        outputs=model(images)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        test_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "    test_accuracy=test_accuracy/len(test_dataloader_norm.dataset)\n",
    "    \n",
    "    print(f\"Loss: {epoch_loss:.4f}  Accuracy: {epoch_accuracy:.4f}\")\n",
    "    \n",
    "    #Save the best model\n",
    "    if test_accuracy>best_accuracy:\n",
    "        torch.save(model.state_dict(),'resnet18_checkpoint.model')\n",
    "        best_accuracy=test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6a4de7-3a4e-4e3a-9f72-8e24de60bd06",
   "metadata": {},
   "source": [
    "## References\n",
    "i. https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "\n",
    "ii. https://github.com/AarohiSingla/Image-Classification-Using-Pytorch/blob/main/image_classification.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
